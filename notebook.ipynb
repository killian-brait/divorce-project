{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the dataset\n",
    "data = pd.read_csv('data/divorce_data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170,\n",
       " 55,\n",
       " 0,\n",
       " Divorce\n",
       " 0    86\n",
       " 1    84\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape of the dataset\n",
    "num_rows, num_cols = data.shape\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum().sum()\n",
    "\n",
    "# Check the balance of the target variable\n",
    "divorce_counts = data['Divorce'].value_counts()\n",
    "\n",
    "num_rows, num_cols, missing_values, divorce_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 170 rows (i.e., couples) and 55 columns (54 predictors and 1 target). There are no missing values in the dataset, which is good as it simplifies the preprocessing steps.\n",
    "\n",
    "The target variable \"Divorce\" is fairly balanced with 86 instances of non-divorced couples (value 0) and 84 instances of divorced couples (value 1). This is beneficial because imbalanced datasets can often lead to biased models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136, 54), (34, 54), (136,), (34,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "features = data.drop('Divorce', axis=1)\n",
    "target = data['Divorce']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "features_train.shape, features_test.shape, target_train.shape, target_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been successfully split into training and test sets. We have 136 instances in the training set and 34 instances in the test set. Each instance has 54 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we'll use a decision tree-based method to rank the importance of the features in predicting divorce. This will help us identify the key predictors of divorce.\n",
    "\n",
    "We'll use the Random Forest algorithm from scikit-learn for this. A Random Forest is an ensemble of Decision Trees that is often used for feature selection because it provides a measure of the importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q40</th>\n",
       "      <th>Q17</th>\n",
       "      <th>Q18</th>\n",
       "      <th>Q19</th>\n",
       "      <th>Q12</th>\n",
       "      <th>Q20</th>\n",
       "      <th>Q16</th>\n",
       "      <th>Q11</th>\n",
       "      <th>Q15</th>\n",
       "      <th>Q26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Q40  Q17  Q18  Q19  Q12  Q20  Q16  Q11  Q15  Q26\n",
       "69     0    4    4    4    4    4    4    4    4    4\n",
       "138    0    0    0    0    0    0    0    0    0    0\n",
       "2      3    3    3    3    4    2    3    3    3    2\n",
       "93     0    0    0    0    0    0    0    0    0    0\n",
       "136    0    1    0    0    1    0    0    0    0    0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf.fit(features_train, target_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame of features and importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importances\n",
    "\n",
    "# Select the top 10 features\n",
    "top_features = feature_importances['Feature'][:10].tolist()\n",
    "\n",
    "# Select these top features from the training and test data\n",
    "features_train_selected = features_train[top_features]\n",
    "features_test_selected = features_test[top_features]\n",
    "\n",
    "features_train_selected.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest has ranked the features by their importance in predicting the target variable \"Divorce\".\n",
    "\n",
    "The five most important features, according to this model, are:\n",
    "\n",
    "Q40 with an importance of approximately 0.0966\n",
    "\n",
    "Q17 with an importance of approximately 0.0951\n",
    "\n",
    "Q18 with an importance of approximately 0.0920\n",
    "\n",
    "Q19 with an importance of approximately 0.0896\n",
    "\n",
    "Q12 with an importance of approximately 0.0896\n",
    "\n",
    "These results suggest that these questions may be particularly important in predicting divorce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "As a starting point, let's choose the top 10 features. However, we can adjust this number later if necessary. Now, let's select these top features from our training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q40</th>\n",
       "      <th>Q17</th>\n",
       "      <th>Q18</th>\n",
       "      <th>Q19</th>\n",
       "      <th>Q12</th>\n",
       "      <th>Q20</th>\n",
       "      <th>Q16</th>\n",
       "      <th>Q11</th>\n",
       "      <th>Q15</th>\n",
       "      <th>Q26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Q40  Q17  Q18  Q19  Q12  Q20  Q16  Q11  Q15  Q26\n",
       "69     0    4    4    4    4    4    4    4    4    4\n",
       "138    0    0    0    0    0    0    0    0    0    0\n",
       "2      3    3    3    3    4    2    3    3    3    2\n",
       "93     0    0    0    0    0    0    0    0    0    0\n",
       "136    0    1    0    0    1    0    0    0    0    0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the top 10 features\n",
    "top_features = feature_importances['Feature'][:10].tolist()\n",
    "\n",
    "# Select these top features from the training and test data\n",
    "features_train_selected = features_train[top_features]\n",
    "features_test_selected = features_test[top_features]\n",
    "\n",
    "features_train_selected.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing k-Nearest Neighbors with Scikt Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train and evaluate a k-NN model using scikit-learn, we can use the KNeighborsClassifier class. After training the model, we can use it to make predictions on the test set, and then compute accuracy and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9705882352941176, 0.9743589743589743)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Initialize the KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the model to the training data\n",
    "knn.fit(features_train_selected, target_train)\n",
    "\n",
    "# Predict the target for the test data\n",
    "target_pred_knn = knn.predict(features_test_selected)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_knn = accuracy_score(target_test, target_pred_knn)\n",
    "\n",
    "# Compute F1 score\n",
    "f1_knn = f1_score(target_test, target_pred_knn)\n",
    "\n",
    "accuracy_knn, f1_knn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-Nearest Neighbors (k-NN) model from scikit-learn achieved an accuracy of approximately 0.971 (or 97.1%) and an F1 score of approximately 0.974 on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing k-Nearest Neighbors from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to implement the k-NN algorithm from scratch. The steps are as follows:\n",
    "\n",
    "1.) Calculate Euclidean distance between two instances.\n",
    "\n",
    "\n",
    "2.) Get the k nearest neighbors of a given test instance.\n",
    "\n",
    "3.) Predict the class of the test instance by taking the mode of the class labels of the k nearest neighbors.\n",
    "\n",
    "Let's start by defining the function for calculating the Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting our DataFrames to numpy arrays will make life easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before conversion to Numpy arrays:\n",
      "features_train:\n",
      "     Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  ...  Q45  Q46  Q47  Q48  Q49  \\\n",
      "69    4   4   4   3   4   2   4   4   4    3  ...    4    0    4    4    4   \n",
      "138   0   0   1   0   0   0   0   1   1    0  ...    3    3    3    3    0   \n",
      "2     2   2   2   2   1   3   2   1   1    2  ...    2    3    2    3    1   \n",
      "93    0   1   0   1   0   0   0   0   0    1  ...    1    1    1    2    1   \n",
      "136   0   0   2   0   0   0   0   0   0    0  ...    2    3    1    2    1   \n",
      "\n",
      "     Q50  Q51  Q52  Q53  Q54  \n",
      "69     3    4    4    4    4  \n",
      "138    1    3    3    3    1  \n",
      "2      1    1    2    2    2  \n",
      "93     1    1    0    0    0  \n",
      "136    2    1    2    2    0  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "target_train:\n",
      "69     1\n",
      "138    0\n",
      "2      1\n",
      "93     0\n",
      "136    0\n",
      "Name: Divorce, dtype: int64\n",
      "features_test:\n",
      "     Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  ...  Q45  Q46  Q47  Q48  Q49  \\\n",
      "139   3   1   1   0   0   0   0   0   0    0  ...    3    3    2    2    0   \n",
      "30    3   4   3   2   3   0   1   4   3    2  ...    4    4    4    4    4   \n",
      "119   0   1   1   0   0   2   0   0   0    0  ...    2    2    2    2    0   \n",
      "29    4   3   3   2   4   1   0   3   3    2  ...    4    4    4    4    4   \n",
      "144   0   0   2   4   0   0   0   0   0    2  ...    3    2    0    2    4   \n",
      "\n",
      "     Q50  Q51  Q52  Q53  Q54  \n",
      "139    2    2    0    0    4  \n",
      "30     4    4    4    4    4  \n",
      "119    2    1    1    1    0  \n",
      "29     4    4    4    4    4  \n",
      "144    0    0    1    0    0  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "target_test:\n",
      "139    0\n",
      "30     1\n",
      "119    0\n",
      "29     1\n",
      "144    0\n",
      "Name: Divorce, dtype: int64\n",
      "\n",
      "Data after conversion to Numpy arrays:\n",
      "X_train:\n",
      "[[4 4 4 ... 4 4 4]\n",
      " [0 0 1 ... 3 3 1]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [3 4 3 ... 4 4 4]\n",
      " [1 1 0 ... 1 1 1]\n",
      " [0 0 0 ... 4 1 2]]\n",
      "y_train:\n",
      "[1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0\n",
      " 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1\n",
      " 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 1\n",
      " 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0]\n",
      "X_test:\n",
      "[[3 1 1 ... 0 0 4]\n",
      " [3 4 3 ... 4 4 4]\n",
      " [0 1 1 ... 1 1 0]\n",
      " ...\n",
      " [3 4 3 ... 3 3 3]\n",
      " [0 3 2 ... 2 0 1]\n",
      " [1 1 1 ... 1 1 0]]\n",
      "y_test:\n",
      "[0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = features_train.to_numpy()\n",
    "y_train = target_train.to_numpy()\n",
    "X_test = features_test.to_numpy()\n",
    "y_test = target_test.to_numpy()\n",
    "\n",
    "print(\"Data before conversion to Numpy arrays:\")\n",
    "print(\"features_train:\")\n",
    "print(features_train.head())\n",
    "print(\"target_train:\")\n",
    "print(target_train.head())\n",
    "print(\"features_test:\")\n",
    "print(features_test.head())\n",
    "print(\"target_test:\")\n",
    "print(target_test.head())\n",
    "\n",
    "print(\"\\nData after conversion to Numpy arrays:\")\n",
    "print(\"X_train:\")\n",
    "print(X_train)\n",
    "print(\"y_train:\")\n",
    "print(y_train)\n",
    "print(\"X_test:\")\n",
    "print(X_test)\n",
    "print(\"y_test:\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance between the first and second training instance: 21.97726097583591\n"
     ]
    }
   ],
   "source": [
    "def calculate_euclidean_distance(instance1, instance2):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two instances.\n",
    "    - instance1: first instance\n",
    "    - instance2: second instance\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((instance1 - instance2) ** 2)) # subtracts corresponding elements of the two arrays\n",
    "\n",
    "print(f\"Euclidean distance between the first and second training instance: {calculate_euclidean_distance(X_train[0], X_train[1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll implement the function to get the k nearest neighbors of a given test instance. This function will compute the Euclidean distance from the test instance to each training instance, keep track of the k instances with the smallest distances, and return their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of the 3 nearest neighbors of the first test instance: [122  89  33]\n"
     ]
    }
   ],
   "source": [
    "def get_k_nearest_neighbors(X_train, x_test, k):\n",
    "    \"\"\"\n",
    "    Get the k nearest neighbors of a test instance.\n",
    "    - X_train: training features\n",
    "    - x_test: test instance\n",
    "    - k: number of neighbors to return\n",
    "    \"\"\"\n",
    "    # Calculate the Euclidean distance from the test instance to each training instance\n",
    "    distances = np.array([calculate_euclidean_distance(x_train, x_test) for x_train in X_train])\n",
    "    \n",
    "    # Get the indices of the k training instances with the smallest distances\n",
    "    nearest_neighbors = distances.argsort()[:k]\n",
    "    \n",
    "    return nearest_neighbors\n",
    "\n",
    "# Test the function\n",
    "print(f\"Indices of the 3 nearest neighbors of the first test instance: {get_k_nearest_neighbors(X_train, X_test[0], 3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement the function to predict the class of a test instance. This function will get the k nearest neighbors of the test instance, find the most common class label among these neighbors, and return this class label as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0, Actual class: 0\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "def predict_with_k_nearest_neighbors(X_train, y_train, x_test, k):\n",
    "    \"\"\"\n",
    "    Predict the class of a test instance using the k nearest neighbors.\n",
    "    - X_train: training features\n",
    "    - y_train: training target values\n",
    "    - x_test: test instance\n",
    "    - k: number of neighbors to consider\n",
    "    \"\"\"\n",
    "    # Get the k nearest neighbors of the test instance\n",
    "    nearest_neighbors = get_k_nearest_neighbors(X_train, x_test, k)\n",
    "    \n",
    "    # Get the class labels of the nearest neighbors\n",
    "    class_labels = y_train[nearest_neighbors]\n",
    "   \n",
    "    # Predict the most common class label\n",
    "    prediction = stats.mode(class_labels)[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test the function\n",
    "prediction = predict_with_k_nearest_neighbors(X_train, y_train, X_test[0], 3)\n",
    "print(f\"Predicted class: {prediction}, Actual class: {target_test.values[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to use this function to make predictions for multiple test instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [0, 1, 0, 1, 0, 0, 0, 1, 0, 1], Actual classes: [0 1 0 1 0 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "def predict_with_k_nearest_neighbors_multiple(X_train, y_train, X_test, k):\n",
    "    \"\"\"\n",
    "    Predict the class of multiple test instances using the k nearest neighbors.\n",
    "    - X_train: training features\n",
    "    - y_train: training target values\n",
    "    - X_test: test features\n",
    "    - k: number of neighbors to consider\n",
    "    \"\"\"\n",
    "    # Make predictions for each test instance\n",
    "    predictions = [predict_with_k_nearest_neighbors(X_train, y_train, x_test, k) for x_test in X_test]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test the function\n",
    "predictions = predict_with_k_nearest_neighbors_multiple(X_train, y_train, X_test, 3)\n",
    "print(f\"Predicted classes: {predictions[:10]}, Actual classes: {y_test[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom implementation of the k-Nearest Neighbors (k-NN) algorithm is working correctly. It made correct predictions for the first 10 instances in the test set!\n",
    "\n",
    "\n",
    "Now that we have implemented and tested the k-NN algorithm from scratch, let's evaluate its performance on the entire test set. We'll compute the accuracy and F1 score as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9705882352941176, 0.9743589743589743)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions for the entire test set\n",
    "predictions = predict_with_k_nearest_neighbors_multiple(X_train, y_train, X_test, 3)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_knn_custom = accuracy_score(y_test, predictions)\n",
    "\n",
    "# Compute F1 score\n",
    "f1_knn_custom = f1_score(y_test, predictions)\n",
    "\n",
    "accuracy_knn_custom, f1_knn_custom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom implementation of the k-Nearest Neighbors (k-NN) algorithm achieved an accuracy of approximately 0.971 (or 97.1%) and an F1 score of approximately 0.974 on the test data. These values are quite high and turned out to be the same as the scikit-learn k-NN. Now, let's compute the permutation feature importance for both the scikit-learn and custom k-NN models.\n",
    "\n",
    "# Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
